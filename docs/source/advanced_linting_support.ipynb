{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced type-checking using linters\n",
    "## Functions that do not affect the schema\n",
    "\n",
    "There are a number of functions in `DataSet` which do not affect the schema. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.Builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typedspark import Column, Schema, DataSet, create_partially_filled_dataset\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "class A(Schema):\n",
    "    a: Column[StringType]\n",
    "\n",
    "df = create_partially_filled_dataset(spark, A, {A.a: [\"a\", \"b\", \"c\"]})\n",
    "res = df.filter(A.a == \"a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, `filter()` will not actually make any changes to the schema, hence we have implemented the return type of `DataSet.filter()` to be a `DataSet` of the same `Schema` that you started with. In other words, a linter will see that `res` is of the type `DataSet[A]`.\n",
    "\n",
    "This allows you to skip casting steps in many cases and instead define functions as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(df: DataSet[A]) -> DataSet[A]:\n",
    "    return df.filter(A.a == \"a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions for which this is currently implemented include:\n",
    "\n",
    "* `filter()`\n",
    "* `distinct()`\n",
    "* `orderBy()`\n",
    "\n",
    "## Functions applied to two DataSets of the same schema\n",
    "\n",
    "Similarly, some functions return a `DataSet[A]` when they take two `DataSet[A]` as an input. For example, here a linter will see that `res` is of the type `DataSet[A]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = create_partially_filled_dataset(spark, A, {A.a: [\"a\", \"b\", \"c\"]})\n",
    "df_b = create_partially_filled_dataset(spark, A, {A.a: [\"d\", \"e\", \"f\"]})\n",
    "\n",
    "res = df_a.unionByName(df_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in this category include:\n",
    "\n",
    "* `unionByName()`\n",
    "* `join(..., how=\"semi\")`\n",
    "\n",
    "## Transformations\n",
    "\n",
    "Finally, the `transform()` function can also be typed. In the following example, a linter will see that `res` is of the type `DataSet[B]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typedspark import transform_to_schema\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "class B(A):\n",
    "    b: Column[StringType]\n",
    "\n",
    "def foo(df: DataSet[A]) -> DataSet[A]:\n",
    "    return transform_to_schema(\n",
    "        df,\n",
    "        B,\n",
    "        {\n",
    "            B.b: lit(\"hi\")\n",
    "        }\n",
    "    )\n",
    "\n",
    "res = (\n",
    "    create_partially_filled_dataset(spark, A, {A.a: [\"a\", \"b\", \"c\"]})\n",
    "    .transform(foo)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did we miss anything?\n",
    "\n",
    "There are likely more functions that we did not yet cover. Feel free to make an issue and reach out when you find one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typedspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
