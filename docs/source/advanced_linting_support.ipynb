{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b83251c2",
   "metadata": {
    "papermill": {
     "duration": 0.00282,
     "end_time": "2023-04-17T15:09:41.947810",
     "exception": false,
     "start_time": "2023-04-17T15:09:41.944990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Advanced type-checking using linters\n",
    "## Functions that do not affect the schema\n",
    "\n",
    "There are a number of functions in `DataSet` which do not affect the schema. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf75d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:09:41.952818Z",
     "iopub.status.busy": "2023-04-17T15:09:41.952623Z",
     "iopub.status.idle": "2023-04-17T15:09:48.563366Z",
     "shell.execute_reply": "2023-04-17T15:09:48.563022Z"
    },
    "papermill": {
     "duration": 6.614384,
     "end_time": "2023-04-17T15:09:48.564409",
     "exception": false,
     "start_time": "2023-04-17T15:09:41.950025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.Builder().config(\"spark.ui.showConsoleProgress\", \"false\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84b26e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:09:48.567589Z",
     "iopub.status.busy": "2023-04-17T15:09:48.567455Z",
     "iopub.status.idle": "2023-04-17T15:09:49.868250Z",
     "shell.execute_reply": "2023-04-17T15:09:49.867887Z"
    },
    "papermill": {
     "duration": 1.303523,
     "end_time": "2023-04-17T15:09:49.869396",
     "exception": false,
     "start_time": "2023-04-17T15:09:48.565873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typedspark import Column, Schema, DataSet, create_partially_filled_dataset\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "class A(Schema):\n",
    "    a: Column[StringType]\n",
    "\n",
    "df = create_partially_filled_dataset(spark, A, {A.a: [\"a\", \"b\", \"c\"]})\n",
    "res = df.filter(A.a == \"a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01675118",
   "metadata": {
    "papermill": {
     "duration": 0.001128,
     "end_time": "2023-04-17T15:09:49.872010",
     "exception": false,
     "start_time": "2023-04-17T15:09:49.870882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the above example, `filter()` will not actually make any changes to the schema, hence we have implemented the return type of `DataSet.filter()` to be a `DataSet` of the same `Schema` that you started with. In other words, a linter will see that `res` is of the type `DataSet[A]`.\n",
    "\n",
    "This allows you to skip casting steps in many cases and instead define functions as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fe9344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:09:49.874895Z",
     "iopub.status.busy": "2023-04-17T15:09:49.874762Z",
     "iopub.status.idle": "2023-04-17T15:09:49.876661Z",
     "shell.execute_reply": "2023-04-17T15:09:49.876409Z"
    },
    "papermill": {
     "duration": 0.004388,
     "end_time": "2023-04-17T15:09:49.877527",
     "exception": false,
     "start_time": "2023-04-17T15:09:49.873139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def foo(df: DataSet[A]) -> DataSet[A]:\n",
    "    return df.filter(A.a == \"a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ed27a7d",
   "metadata": {
    "papermill": {
     "duration": 0.001032,
     "end_time": "2023-04-17T15:09:49.879767",
     "exception": false,
     "start_time": "2023-04-17T15:09:49.878735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The functions for which this is currently implemented include:\n",
    "\n",
    "* `filter()`\n",
    "* `distinct()`\n",
    "* `orderBy()`\n",
    "\n",
    "## Functions applied to two DataSets of the same schema\n",
    "\n",
    "Similarly, some functions return a `DataSet[A]` when they take two `DataSet[A]` as an input. For example, here a linter will see that `res` is of the type `DataSet[A]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3abfb0d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:09:49.882393Z",
     "iopub.status.busy": "2023-04-17T15:09:49.882282Z",
     "iopub.status.idle": "2023-04-17T15:09:49.929606Z",
     "shell.execute_reply": "2023-04-17T15:09:49.929339Z"
    },
    "papermill": {
     "duration": 0.04976,
     "end_time": "2023-04-17T15:09:49.930566",
     "exception": false,
     "start_time": "2023-04-17T15:09:49.880806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_a = create_partially_filled_dataset(spark, A, {A.a: [\"a\", \"b\", \"c\"]})\n",
    "df_b = create_partially_filled_dataset(spark, A, {A.a: [\"d\", \"e\", \"f\"]})\n",
    "\n",
    "res = df_a.unionByName(df_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfa33e61",
   "metadata": {
    "papermill": {
     "duration": 0.001035,
     "end_time": "2023-04-17T15:09:49.932880",
     "exception": false,
     "start_time": "2023-04-17T15:09:49.931845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The functions in this category include:\n",
    "\n",
    "* `unionByName()`\n",
    "* `join(..., how=\"semi\")`\n",
    "\n",
    "## Transformations\n",
    "\n",
    "Finally, the `transform()` function can also be typed. In the following example, a linter will see that `res` is of the type `DataSet[B]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b0df362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:09:49.935323Z",
     "iopub.status.busy": "2023-04-17T15:09:49.935218Z",
     "iopub.status.idle": "2023-04-17T15:09:49.964137Z",
     "shell.execute_reply": "2023-04-17T15:09:49.963846Z"
    },
    "papermill": {
     "duration": 0.0312,
     "end_time": "2023-04-17T15:09:49.965036",
     "exception": false,
     "start_time": "2023-04-17T15:09:49.933836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typedspark import transform_to_schema\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "class B(A):\n",
    "    b: Column[StringType]\n",
    "\n",
    "def foo(df: DataSet[A]) -> DataSet[A]:\n",
    "    return transform_to_schema(\n",
    "        df,\n",
    "        B,\n",
    "        {\n",
    "            B.b: lit(\"hi\")\n",
    "        }\n",
    "    )\n",
    "\n",
    "res = (\n",
    "    create_partially_filled_dataset(spark, A, {A.a: [\"a\", \"b\", \"c\"]})\n",
    "    .transform(foo)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4221be22",
   "metadata": {
    "papermill": {
     "duration": 0.000986,
     "end_time": "2023-04-17T15:09:49.967196",
     "exception": false,
     "start_time": "2023-04-17T15:09:49.966210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Did we miss anything?\n",
    "\n",
    "There are likely more functions that we did not yet cover. Feel free to make an issue and reach out when you find one!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ad841",
   "metadata": {
    "papermill": {
     "duration": 0.000999,
     "end_time": "2023-04-17T15:09:49.969197",
     "exception": false,
     "start_time": "2023-04-17T15:09:49.968198",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typedspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.638315,
   "end_time": "2023-04-17T15:09:50.693163",
   "environment_variables": {},
   "exception": null,
   "input_path": "docs/source/advanced_linting_support.ipynb",
   "output_path": "docs/source/advanced_linting_support.ipynb",
   "parameters": {},
   "start_time": "2023-04-17T15:09:41.054848",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
