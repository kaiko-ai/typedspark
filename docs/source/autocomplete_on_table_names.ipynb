{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bac925e",
   "metadata": {},
   "source": [
    "# Autocomplete on table names\n",
    "\n",
    "## The basics\n",
    "\n",
    "With `load_table()` we can dynamically load `DataSets` and their `Schemas`. However, we still need to type the table name as a string, which can be cumbersome because of typos or because we forgot the exact name. The classes described here give you autocomplete on table names and hence alleviate these problems. To illustrate this, let's first generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88feb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.Builder().config(\"spark.ui.showConsoleProgress\", \"false\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56535f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanneaben/.pyenv/versions/typedspark/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "(\n",
    "    spark.createDataFrame(\n",
    "        pd.DataFrame(\n",
    "            dict(\n",
    "                name=[\"Jack\", \"John\", \"Jane\"],\n",
    "                age=[20, 30, 40],\n",
    "            )\n",
    "        )\n",
    "    ).createOrReplaceTempView(\"person_table\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa94840a",
   "metadata": {},
   "source": [
    "## Catalogs\n",
    "\n",
    "Using `Catalogs`, we can get autocomplete on all tables that Spark has access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae12618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typedspark import Catalogs\n",
    "\n",
    "db = Catalogs(spark)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "033c28d6",
   "metadata": {},
   "source": [
    "After running the above cell, we can use `db` to load our table. Notice that you'll get autocomplete here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4adf417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons, Person = db.spark_catalog.default.person_table()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03140ede",
   "metadata": {},
   "source": [
    "We can use the `DataSet` and `Schema` just as we would before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e81f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Jack| 20|\n",
      "|John| 30|\n",
      "|Jane| 40|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54ef94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "from pyspark.sql.types import LongType, StringType\n",
       "\n",
       "from typedspark import Column, Schema\n",
       "\n",
       "\n",
       "class PersonTable(Schema):\n",
       "    name: Column[StringType]\n",
       "    age: Column[LongType]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9cee952",
   "metadata": {},
   "source": [
    "## Databases\n",
    "\n",
    "`Catalogs` is often the only class you need. But if loading all catalogs takes too long, or if you only want to use only one catalog anyway, you can use `Databases` instead. We can use `Databases(spark, catalog_name=...)` to specify which catalog we want to load. Or we can omit this parameter to load the default catalog (often `spark_catalog` or `hive_metastore`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac42d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typedspark import Databases\n",
    "\n",
    "db = Databases(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df585e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons, Person = db.default.person_table()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8606fb4",
   "metadata": {},
   "source": [
    "## Database\n",
    "\n",
    "Finally, if we just want to load the tables from a single database, we can use `Database`. Once again, we can either specify the database (through `Database(spark, db_name=...)`) or leave it blank to load the default database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1233a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typedspark import Database\n",
    "\n",
    "db = Database(spark, db_name=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b2c02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "person, Person = db.person_table.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typedspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
