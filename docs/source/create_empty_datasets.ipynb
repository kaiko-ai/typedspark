{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f25abb69",
   "metadata": {},
   "source": [
    "# Easier unit testing through the creation of empty DataSets from schemas\n",
    "We provide helper functions to generate (partially) empty DataSets from existing schemas. This can be helpful in certain situations, such as unit testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b679fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.Builder().config(\"spark.ui.showConsoleProgress\", \"false\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2690c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  id|name| age|\n",
      "+----+----+----+\n",
      "|null|null|null|\n",
      "|null|null|null|\n",
      "|null|null|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typedspark import Column, Schema, create_empty_dataset, create_partially_filled_dataset\n",
    "from pyspark.sql.types import LongType, StringType\n",
    "\n",
    "class Person(Schema):\n",
    "    id: Column[LongType]\n",
    "    name: Column[StringType]\n",
    "    age: Column[LongType]\n",
    "\n",
    "df_empty = create_empty_dataset(spark, Person)\n",
    "df_empty.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ce76e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|name| age|\n",
      "+---+----+----+\n",
      "|  1|John|null|\n",
      "|  2|Jane|null|\n",
      "|  3|Jack|null|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partially_filled = create_partially_filled_dataset(\n",
    "    spark,\n",
    "    Person,\n",
    "    {\n",
    "        Person.id: [1, 2, 3],\n",
    "        Person.name: [\"John\", \"Jane\", \"Jack\"],\n",
    "    }\n",
    ")\n",
    "df_partially_filled.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74e41042",
   "metadata": {},
   "source": [
    "## Complex datatypes\n",
    "\n",
    "This trick also works for more complex data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3578c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+---------+\n",
      "|     a|               b|        c|\n",
      "+------+----------------+---------+\n",
      "|   [a]|        {a -> 1}|{a, null}|\n",
      "|[b, c]|{b -> 2, c -> 3}|{b, null}|\n",
      "|   [d]|        {d -> 4}|{c, null}|\n",
      "+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typedspark import ArrayType, MapType, StructType\n",
    "\n",
    "class A(Schema):\n",
    "    a: Column[StringType]\n",
    "    b: Column[StringType]\n",
    "\n",
    "class B(Schema):\n",
    "    a: Column[ArrayType[StringType]]\n",
    "    b: Column[MapType[StringType, StringType]]\n",
    "    c: Column[StructType[A]]\n",
    "\n",
    "df_a = create_partially_filled_dataset(\n",
    "    spark, A, {\n",
    "        A.a: [\"a\", \"b\", \"c\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "df_b = create_partially_filled_dataset(\n",
    "    spark, B, {\n",
    "        B.a: [[\"a\"], [\"b\", \"c\"], [\"d\"]],\n",
    "        B.b: [{\"a\": \"1\"}, {\"b\": \"2\", \"c\": \"3\"}, {\"d\": \"4\"}],\n",
    "        B.c: df_a.collect(),\n",
    "    }\n",
    ")\n",
    "df_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d3b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---+\n",
      "|         a|                  b|  c|\n",
      "+----------+-------------------+---+\n",
      "|2020-01-01|2020-01-01 10:15:00| 32|\n",
      "+----------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "from decimal import Decimal\n",
    "from pyspark.sql.types import DateType, DecimalType, TimestampType\n",
    "\n",
    "class C(Schema):\n",
    "    a: Column[DateType]\n",
    "    b: Column[TimestampType]\n",
    "    c: Column[DecimalType]\n",
    "\n",
    "create_partially_filled_dataset(\n",
    "    spark, \n",
    "    C, \n",
    "    {\n",
    "        C.a: [date(2020, 1, 1)],\n",
    "        C.b: [datetime(2020, 1, 1, 10, 15)],\n",
    "        C.c: [Decimal(32)],\n",
    "    }\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "571cf714",
   "metadata": {},
   "source": [
    "## Row-wise definition of your DataSets\n",
    "\n",
    "It is also possible to define your DataSets in a row-wise fashion. This is useful for cases where you have few columns, but many rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68e52bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+\n",
      "|  id|   name|age|\n",
      "+----+-------+---+\n",
      "|null|  Alice| 20|\n",
      "|null|    Bob| 30|\n",
      "|null|Charlie| 40|\n",
      "|null|   Dave| 50|\n",
      "|null|    Eve| 60|\n",
      "|null|  Frank| 70|\n",
      "|null|  Grace| 80|\n",
      "+----+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_partially_filled_dataset(\n",
    "    spark, \n",
    "    Person, \n",
    "    [\n",
    "        {Person.name: \"Alice\", Person.age: 20},\n",
    "        {Person.name: \"Bob\", Person.age: 30},\n",
    "        {Person.name: \"Charlie\", Person.age: 40},\n",
    "        {Person.name: \"Dave\", Person.age: 50},\n",
    "        {Person.name: \"Eve\", Person.age: 60},\n",
    "        {Person.name: \"Frank\", Person.age: 70},\n",
    "        {Person.name: \"Grace\", Person.age: 80},\n",
    "    ]\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8621aaf1",
   "metadata": {},
   "source": [
    "This format also works for the complex data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb8ab22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+------+\n",
      "|     a|               b|     c|\n",
      "+------+----------------+------+\n",
      "|   [a]|        {a -> 1}|{a, b}|\n",
      "|[b, c]|{b -> 2, c -> 3}|{b, c}|\n",
      "|   [d]|        {d -> 4}|{d, e}|\n",
      "+------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typedspark import create_structtype_row\n",
    "\n",
    "create_partially_filled_dataset(\n",
    "    spark,\n",
    "    B,\n",
    "    [\n",
    "        {\n",
    "            B.a: [\"a\"], \n",
    "            B.b: {\"a\": \"1\"}, \n",
    "            B.c: create_structtype_row(A, {A.a: \"a\", A.b: \"b\"}),\n",
    "        },\n",
    "        {\n",
    "            B.a: [\"b\", \"c\"],\n",
    "            B.b: {\"b\": \"2\", \"c\": \"3\"},\n",
    "            B.c: create_structtype_row(A, {A.a: \"b\", A.b: \"c\"}),\n",
    "        },\n",
    "        {\n",
    "            B.a: [\"d\"],\n",
    "            B.b: {\"d\": \"4\"},\n",
    "            B.c: create_structtype_row(A, {A.a: \"d\", A.b: \"e\"}),\n",
    "        },\n",
    "    ],\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd5c7c1c",
   "metadata": {},
   "source": [
    "## Example unit test\n",
    "\n",
    "The following code snippet shows what a full unit test using typedspark can look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80ddebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, StringType\n",
    "from typedspark import Column, DataSet, Schema, create_partially_filled_dataset, transform_to_schema\n",
    "from chispa.dataframe_comparer import assert_df_equality\n",
    "\n",
    "\n",
    "class Person(Schema):\n",
    "    name: Column[StringType]\n",
    "    age: Column[LongType]\n",
    "\n",
    "\n",
    "def birthday(df: DataSet[Person]) -> DataSet[Person]:\n",
    "    return transform_to_schema(df, Person, {Person.age: Person.age + 1})\n",
    "\n",
    "\n",
    "def test_birthday(spark: SparkSession):\n",
    "    df = create_partially_filled_dataset(\n",
    "        spark, Person, {Person.name: [\"Alice\", \"Bob\"], Person.age: [20, 30]}\n",
    "    )\n",
    "\n",
    "    observed = birthday(df)\n",
    "    expected = create_partially_filled_dataset(\n",
    "        spark, Person, {Person.name: [\"Alice\", \"Bob\"], Person.age: [21, 31]}\n",
    "    )\n",
    "\n",
    "    assert_df_equality(observed, expected, ignore_row_order=True, ignore_nullable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e716a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typedspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
