{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f25abb69",
   "metadata": {
    "papermill": {
     "duration": 0.011439,
     "end_time": "2023-04-17T15:10:16.291714",
     "exception": false,
     "start_time": "2023-04-17T15:10:16.280275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Easier unit testing through the creation of empty DataSets from schemas\n",
    "We provide helper functions to generate (partially) empty DataSets from existing schemas. This can be helpful in certain situations, such as unit testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b679fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:10:16.303916Z",
     "iopub.status.busy": "2023-04-17T15:10:16.303577Z",
     "iopub.status.idle": "2023-04-17T15:10:22.920830Z",
     "shell.execute_reply": "2023-04-17T15:10:22.920539Z"
    },
    "papermill": {
     "duration": 6.624263,
     "end_time": "2023-04-17T15:10:22.921881",
     "exception": false,
     "start_time": "2023-04-17T15:10:16.297618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.Builder().config(\"spark.ui.showConsoleProgress\", \"false\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2690c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:10:22.925670Z",
     "iopub.status.busy": "2023-04-17T15:10:22.925514Z",
     "iopub.status.idle": "2023-04-17T15:10:25.283278Z",
     "shell.execute_reply": "2023-04-17T15:10:25.282919Z"
    },
    "papermill": {
     "duration": 2.360643,
     "end_time": "2023-04-17T15:10:25.284256",
     "exception": false,
     "start_time": "2023-04-17T15:10:22.923613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  id|name| age|\n",
      "+----+----+----+\n",
      "|null|null|null|\n",
      "|null|null|null|\n",
      "|null|null|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typedspark import Column, Schema, create_empty_dataset, create_partially_filled_dataset\n",
    "from pyspark.sql.types import LongType, StringType\n",
    "\n",
    "class Person(Schema):\n",
    "    id: Column[LongType]\n",
    "    name: Column[StringType]\n",
    "    age: Column[LongType]\n",
    "\n",
    "df_empty = create_empty_dataset(spark, Person)\n",
    "df_empty.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ce76e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:10:25.287919Z",
     "iopub.status.busy": "2023-04-17T15:10:25.287786Z",
     "iopub.status.idle": "2023-04-17T15:10:25.515863Z",
     "shell.execute_reply": "2023-04-17T15:10:25.515567Z"
    },
    "papermill": {
     "duration": 0.231059,
     "end_time": "2023-04-17T15:10:25.517035",
     "exception": false,
     "start_time": "2023-04-17T15:10:25.285976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|name| age|\n",
      "+---+----+----+\n",
      "|  1|John|null|\n",
      "|  2|Jane|null|\n",
      "|  3|Jack|null|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partially_filled = create_partially_filled_dataset(\n",
    "    spark,\n",
    "    Person,\n",
    "    {\n",
    "        Person.id: [1, 2, 3],\n",
    "        Person.name: [\"John\", \"Jane\", \"Jack\"],\n",
    "    }\n",
    ")\n",
    "df_partially_filled.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74e41042",
   "metadata": {
    "papermill": {
     "duration": 0.001411,
     "end_time": "2023-04-17T15:10:25.520236",
     "exception": false,
     "start_time": "2023-04-17T15:10:25.518825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Complex datatypes\n",
    "\n",
    "This trick also works for more complex data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3578c0e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:10:25.523706Z",
     "iopub.status.busy": "2023-04-17T15:10:25.523488Z",
     "iopub.status.idle": "2023-04-17T15:10:25.957968Z",
     "shell.execute_reply": "2023-04-17T15:10:25.957695Z"
    },
    "papermill": {
     "duration": 0.437352,
     "end_time": "2023-04-17T15:10:25.958948",
     "exception": false,
     "start_time": "2023-04-17T15:10:25.521596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+---------+\n",
      "|     a|               b|        c|\n",
      "+------+----------------+---------+\n",
      "|   [a]|        {a -> 1}|{a, null}|\n",
      "|[b, c]|{b -> 2, c -> 3}|{b, null}|\n",
      "|   [d]|        {d -> 4}|{c, null}|\n",
      "+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typedspark import ArrayType, MapType, StructType\n",
    "\n",
    "class A(Schema):\n",
    "    a: Column[StringType]\n",
    "    b: Column[StringType]\n",
    "\n",
    "class B(Schema):\n",
    "    a: Column[ArrayType[StringType]]\n",
    "    b: Column[MapType[StringType, StringType]]\n",
    "    c: Column[StructType[A]]\n",
    "\n",
    "df_a = create_partially_filled_dataset(\n",
    "    spark, A, {\n",
    "        A.a: [\"a\", \"b\", \"c\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "df_b = create_partially_filled_dataset(\n",
    "    spark, B, {\n",
    "        B.a: [[\"a\"], [\"b\", \"c\"], [\"d\"]],\n",
    "        B.b: [{\"a\": \"1\"}, {\"b\": \"2\", \"c\": \"3\"}, {\"d\": \"4\"}],\n",
    "        B.c: df_a.collect(),\n",
    "    }\n",
    ")\n",
    "df_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d3b302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:10:25.962879Z",
     "iopub.status.busy": "2023-04-17T15:10:25.962759Z",
     "iopub.status.idle": "2023-04-17T15:10:26.199825Z",
     "shell.execute_reply": "2023-04-17T15:10:26.199523Z"
    },
    "papermill": {
     "duration": 0.239937,
     "end_time": "2023-04-17T15:10:26.200697",
     "exception": false,
     "start_time": "2023-04-17T15:10:25.960760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---+\n",
      "|         a|                  b|  c|\n",
      "+----------+-------------------+---+\n",
      "|2020-01-01|2020-01-01 10:15:00| 32|\n",
      "+----------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "from decimal import Decimal\n",
    "from pyspark.sql.types import DateType, DecimalType, TimestampType\n",
    "\n",
    "class C(Schema):\n",
    "    a: Column[DateType]\n",
    "    b: Column[TimestampType]\n",
    "    c: Column[DecimalType]\n",
    "\n",
    "create_partially_filled_dataset(\n",
    "    spark, \n",
    "    C, \n",
    "    {\n",
    "        C.a: [date(2020, 1, 1)],\n",
    "        C.b: [datetime(2020, 1, 1, 10, 15)],\n",
    "        C.c: [Decimal(32)],\n",
    "    }\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "571cf714",
   "metadata": {
    "papermill": {
     "duration": 0.00137,
     "end_time": "2023-04-17T15:10:26.203761",
     "exception": false,
     "start_time": "2023-04-17T15:10:26.202391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Row-wise definition of your DataSets\n",
    "\n",
    "It is also possible to define your DataSets in a row-wise fashion. This is useful for cases where you have few columns, but many rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68e52bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:10:26.206985Z",
     "iopub.status.busy": "2023-04-17T15:10:26.206873Z",
     "iopub.status.idle": "2023-04-17T15:10:26.434774Z",
     "shell.execute_reply": "2023-04-17T15:10:26.434487Z"
    },
    "papermill": {
     "duration": 0.230564,
     "end_time": "2023-04-17T15:10:26.435674",
     "exception": false,
     "start_time": "2023-04-17T15:10:26.205110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+\n",
      "|  id|   name|age|\n",
      "+----+-------+---+\n",
      "|null|  Alice| 20|\n",
      "|null|    Bob| 30|\n",
      "|null|Charlie| 40|\n",
      "|null|   Dave| 50|\n",
      "|null|    Eve| 60|\n",
      "|null|  Frank| 70|\n",
      "|null|  Grace| 80|\n",
      "+----+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_partially_filled_dataset(\n",
    "    spark, \n",
    "    Person, \n",
    "    [\n",
    "        {Person.name: \"Alice\", Person.age: 20},\n",
    "        {Person.name: \"Bob\", Person.age: 30},\n",
    "        {Person.name: \"Charlie\", Person.age: 40},\n",
    "        {Person.name: \"Dave\", Person.age: 50},\n",
    "        {Person.name: \"Eve\", Person.age: 60},\n",
    "        {Person.name: \"Frank\", Person.age: 70},\n",
    "        {Person.name: \"Grace\", Person.age: 80},\n",
    "    ]\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8621aaf1",
   "metadata": {
    "papermill": {
     "duration": 0.001424,
     "end_time": "2023-04-17T15:10:26.438866",
     "exception": false,
     "start_time": "2023-04-17T15:10:26.437442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This format also works for the complex data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb8ab22f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:10:26.442826Z",
     "iopub.status.busy": "2023-04-17T15:10:26.442711Z",
     "iopub.status.idle": "2023-04-17T15:10:26.665659Z",
     "shell.execute_reply": "2023-04-17T15:10:26.665384Z"
    },
    "papermill": {
     "duration": 0.226339,
     "end_time": "2023-04-17T15:10:26.666529",
     "exception": false,
     "start_time": "2023-04-17T15:10:26.440190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+------+\n",
      "|     a|               b|     c|\n",
      "+------+----------------+------+\n",
      "|   [a]|        {a -> 1}|{a, b}|\n",
      "|[b, c]|{b -> 2, c -> 3}|{b, c}|\n",
      "|   [d]|        {d -> 4}|{d, e}|\n",
      "+------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typedspark import create_structtype_row\n",
    "\n",
    "create_partially_filled_dataset(\n",
    "    spark,\n",
    "    B,\n",
    "    [\n",
    "        {\n",
    "            B.a: [\"a\"], \n",
    "            B.b: {\"a\": \"1\"}, \n",
    "            B.c: create_structtype_row(A, {A.a: \"a\", A.b: \"b\"}),\n",
    "        },\n",
    "        {\n",
    "            B.a: [\"b\", \"c\"],\n",
    "            B.b: {\"b\": \"2\", \"c\": \"3\"},\n",
    "            B.c: create_structtype_row(A, {A.a: \"b\", A.b: \"c\"}),\n",
    "        },\n",
    "        {\n",
    "            B.a: [\"d\"],\n",
    "            B.b: {\"d\": \"4\"},\n",
    "            B.c: create_structtype_row(A, {A.a: \"d\", A.b: \"e\"}),\n",
    "        },\n",
    "    ],\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd5c7c1c",
   "metadata": {
    "papermill": {
     "duration": 0.001484,
     "end_time": "2023-04-17T15:10:26.669844",
     "exception": false,
     "start_time": "2023-04-17T15:10:26.668360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example unit test\n",
    "\n",
    "The following code snippet shows what a full unit test using typedspark can look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80ddebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T15:10:26.673310Z",
     "iopub.status.busy": "2023-04-17T15:10:26.673190Z",
     "iopub.status.idle": "2023-04-17T15:10:26.680377Z",
     "shell.execute_reply": "2023-04-17T15:10:26.680109Z"
    },
    "papermill": {
     "duration": 0.009973,
     "end_time": "2023-04-17T15:10:26.681237",
     "exception": false,
     "start_time": "2023-04-17T15:10:26.671264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, StringType\n",
    "from typedspark import Column, DataSet, Schema, create_partially_filled_dataset, transform_to_schema\n",
    "from chispa.dataframe_comparer import assert_df_equality\n",
    "\n",
    "\n",
    "class Person(Schema):\n",
    "    name: Column[StringType]\n",
    "    age: Column[LongType]\n",
    "\n",
    "\n",
    "def birthday(df: DataSet[Person]) -> DataSet[Person]:\n",
    "    return transform_to_schema(df, Person, {Person.age: Person.age + 1})\n",
    "\n",
    "\n",
    "def test_birthday(spark: SparkSession):\n",
    "    df = create_partially_filled_dataset(\n",
    "        spark, Person, {Person.name: [\"Alice\", \"Bob\"], Person.age: [20, 30]}\n",
    "    )\n",
    "\n",
    "    observed = birthday(df)\n",
    "    expected = create_partially_filled_dataset(\n",
    "        spark, Person, {Person.name: [\"Alice\", \"Bob\"], Person.age: [21, 31]}\n",
    "    )\n",
    "\n",
    "    assert_df_equality(observed, expected, ignore_row_order=True, ignore_nullable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e716a8",
   "metadata": {
    "papermill": {
     "duration": 0.001503,
     "end_time": "2023-04-17T15:10:26.700931",
     "exception": false,
     "start_time": "2023-04-17T15:10:26.699428",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typedspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.079447,
   "end_time": "2023-04-17T15:10:28.455184",
   "environment_variables": {},
   "exception": null,
   "input_path": "docs/source/create_empty_datasets.ipynb",
   "output_path": "docs/source/create_empty_datasets.ipynb",
   "parameters": {},
   "start_time": "2023-04-17T15:10:15.375737",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
