{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f25abb69",
   "metadata": {
    "papermill": {
     "duration": 0.007407,
     "end_time": "2023-04-13T15:20:28.948134",
     "exception": false,
     "start_time": "2023-04-13T15:20:28.940727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Easier unit testing through the creation of empty DataSets from schemas\n",
    "We provide helper functions to generate (partially) empty DataSets from existing schemas. This can be helpful in certain situations, such as unit testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b679fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T15:20:28.955463Z",
     "iopub.status.busy": "2023-04-13T15:20:28.955157Z",
     "iopub.status.idle": "2023-04-13T15:20:35.581396Z",
     "shell.execute_reply": "2023-04-13T15:20:35.581031Z"
    },
    "papermill": {
     "duration": 6.631171,
     "end_time": "2023-04-13T15:20:35.582625",
     "exception": false,
     "start_time": "2023-04-13T15:20:28.951454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.Builder().getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2690c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T15:20:35.586072Z",
     "iopub.status.busy": "2023-04-13T15:20:35.585852Z",
     "iopub.status.idle": "2023-04-13T15:20:37.682067Z",
     "shell.execute_reply": "2023-04-13T15:20:37.681747Z"
    },
    "papermill": {
     "duration": 2.099043,
     "end_time": "2023-04-13T15:20:37.683127",
     "exception": false,
     "start_time": "2023-04-13T15:20:35.584084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  id|name| age|\n",
      "+----+----+----+\n",
      "|null|null|null|\n",
      "|null|null|null|\n",
      "|null|null|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typedspark import Column, Schema, create_empty_dataset, create_partially_filled_dataset\n",
    "from pyspark.sql.types import LongType, StringType\n",
    "\n",
    "class Person(Schema):\n",
    "    id: Column[LongType]\n",
    "    name: Column[StringType]\n",
    "    age: Column[LongType]\n",
    "\n",
    "df_empty = create_empty_dataset(spark, Person)\n",
    "df_empty.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ce76e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T15:20:37.686360Z",
     "iopub.status.busy": "2023-04-13T15:20:37.686213Z",
     "iopub.status.idle": "2023-04-13T15:20:37.799144Z",
     "shell.execute_reply": "2023-04-13T15:20:37.798715Z"
    },
    "papermill": {
     "duration": 0.115959,
     "end_time": "2023-04-13T15:20:37.800521",
     "exception": false,
     "start_time": "2023-04-13T15:20:37.684562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|name| age|\n",
      "+---+----+----+\n",
      "|  1|John|null|\n",
      "|  2|Jane|null|\n",
      "|  3|Jack|null|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partially_filled = create_partially_filled_dataset(\n",
    "    spark,\n",
    "    Person,\n",
    "    {\n",
    "        Person.id: [1, 2, 3],\n",
    "        Person.name: [\"John\", \"Jane\", \"Jack\"],\n",
    "    }\n",
    ")\n",
    "df_partially_filled.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74e41042",
   "metadata": {
    "papermill": {
     "duration": 0.001248,
     "end_time": "2023-04-13T15:20:37.803390",
     "exception": false,
     "start_time": "2023-04-13T15:20:37.802142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Complex datatypes\n",
    "\n",
    "This trick also works for more complex data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3578c0e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T15:20:37.806747Z",
     "iopub.status.busy": "2023-04-13T15:20:37.806562Z",
     "iopub.status.idle": "2023-04-13T15:20:38.058925Z",
     "shell.execute_reply": "2023-04-13T15:20:38.058441Z"
    },
    "papermill": {
     "duration": 0.25546,
     "end_time": "2023-04-13T15:20:38.060055",
     "exception": false,
     "start_time": "2023-04-13T15:20:37.804595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+---------+\n",
      "|     a|               b|        c|\n",
      "+------+----------------+---------+\n",
      "|   [a]|        {a -> 1}|{a, null}|\n",
      "|[b, c]|{b -> 2, c -> 3}|{b, null}|\n",
      "|   [d]|        {d -> 4}|{c, null}|\n",
      "+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typedspark import ArrayType, MapType, StructType\n",
    "\n",
    "class A(Schema):\n",
    "    a: Column[StringType]\n",
    "    b: Column[StringType]\n",
    "\n",
    "class B(Schema):\n",
    "    a: Column[ArrayType[StringType]]\n",
    "    b: Column[MapType[StringType, StringType]]\n",
    "    c: Column[StructType[A]]\n",
    "\n",
    "df_a = create_partially_filled_dataset(\n",
    "    spark, A, {\n",
    "        A.a: [\"a\", \"b\", \"c\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "df_b = create_partially_filled_dataset(\n",
    "    spark, B, {\n",
    "        B.a: [[\"a\"], [\"b\", \"c\"], [\"d\"]],\n",
    "        B.b: [{\"a\": \"1\"}, {\"b\": \"2\", \"c\": \"3\"}, {\"d\": \"4\"}],\n",
    "        B.c: df_a.collect(),\n",
    "    }\n",
    ")\n",
    "df_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d3b302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T15:20:38.063850Z",
     "iopub.status.busy": "2023-04-13T15:20:38.063690Z",
     "iopub.status.idle": "2023-04-13T15:20:38.194434Z",
     "shell.execute_reply": "2023-04-13T15:20:38.194037Z"
    },
    "papermill": {
     "duration": 0.133748,
     "end_time": "2023-04-13T15:20:38.195474",
     "exception": false,
     "start_time": "2023-04-13T15:20:38.061726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---+\n",
      "|         a|                  b|  c|\n",
      "+----------+-------------------+---+\n",
      "|2020-01-01|2020-01-01 10:15:00| 32|\n",
      "+----------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "from decimal import Decimal\n",
    "from pyspark.sql.types import DateType, DecimalType, TimestampType\n",
    "\n",
    "class A(Schema):\n",
    "    a: Column[DateType]\n",
    "    b: Column[TimestampType]\n",
    "    c: Column[DecimalType]\n",
    "\n",
    "create_partially_filled_dataset(\n",
    "    spark, \n",
    "    A, \n",
    "    {\n",
    "        A.a: [date(2020, 1, 1)],\n",
    "        A.b: [datetime(2020, 1, 1, 10, 15)],\n",
    "        A.c: [Decimal(32)],\n",
    "    }\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43fb6f66",
   "metadata": {
    "papermill": {
     "duration": 0.001213,
     "end_time": "2023-04-13T15:20:38.198265",
     "exception": false,
     "start_time": "2023-04-13T15:20:38.197052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example unit test\n",
    "\n",
    "The following code snippet shows what a full unit test using typedspark can look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80ddebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T15:20:38.202940Z",
     "iopub.status.busy": "2023-04-13T15:20:38.202620Z",
     "iopub.status.idle": "2023-04-13T15:20:38.209893Z",
     "shell.execute_reply": "2023-04-13T15:20:38.209522Z"
    },
    "papermill": {
     "duration": 0.01172,
     "end_time": "2023-04-13T15:20:38.211163",
     "exception": false,
     "start_time": "2023-04-13T15:20:38.199443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, StringType\n",
    "from typedspark import Column, DataSet, Schema, create_partially_filled_dataset, transform_to_schema\n",
    "from chispa.dataframe_comparer import assert_df_equality\n",
    "\n",
    "\n",
    "class Person(Schema):\n",
    "    name: Column[StringType]\n",
    "    age: Column[LongType]\n",
    "\n",
    "\n",
    "def birthday(df: DataSet[Person]) -> DataSet[Person]:\n",
    "    return transform_to_schema(df, Person, {Person.age: Person.age + 1})\n",
    "\n",
    "\n",
    "def test_birthday(spark: SparkSession):\n",
    "    df = create_partially_filled_dataset(\n",
    "        spark, Person, {Person.name: [\"Alice\", \"Bob\"], Person.age: [20, 30]}\n",
    "    )\n",
    "\n",
    "    observed = birthday(df)\n",
    "    expected = create_partially_filled_dataset(\n",
    "        spark, Person, {Person.name: [\"Alice\", \"Bob\"], Person.age: [21, 31]}\n",
    "    )\n",
    "\n",
    "    assert_df_equality(observed, expected, ignore_row_order=True, ignore_nullable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571cf714",
   "metadata": {
    "papermill": {
     "duration": 0.001234,
     "end_time": "2023-04-13T15:20:38.214075",
     "exception": false,
     "start_time": "2023-04-13T15:20:38.212841",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typedspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.884884,
   "end_time": "2023-04-13T15:20:38.937714",
   "environment_variables": {},
   "exception": null,
   "input_path": "docs/source/create_empty_datasets.ipynb",
   "output_path": "docs/source/create_empty_datasets.ipynb",
   "parameters": {},
   "start_time": "2023-04-13T15:20:28.052830",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
