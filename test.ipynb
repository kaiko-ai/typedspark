{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPersonWithJob\u001b[39;00m(Person, Job):\n\u001b[1;32m     29\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     32\u001b[0m (\n\u001b[0;32m---> 33\u001b[0m     transform_to_schema(\n\u001b[1;32m     34\u001b[0m         df_a\u001b[39m.\u001b[39;49mjoin(\n\u001b[1;32m     35\u001b[0m             df_b,\n\u001b[1;32m     36\u001b[0m             person\u001b[39m.\u001b[39;49mid \u001b[39m==\u001b[39;49m job\u001b[39m.\u001b[39;49mid,\n\u001b[1;32m     37\u001b[0m         ),\n\u001b[1;32m     38\u001b[0m         PersonWithJob,\n\u001b[1;32m     39\u001b[0m         {\n\u001b[1;32m     40\u001b[0m             PersonWithJob\u001b[39m.\u001b[39;49mid: job\u001b[39m.\u001b[39;49mid,\n\u001b[1;32m     41\u001b[0m         },\n\u001b[1;32m     42\u001b[0m     )\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/projects/2022/typedspark/typedspark/_transforms/transform_to_schema.py:47\u001b[0m, in \u001b[0;36mtransform_to_schema\u001b[0;34m(dataframe, schema, transformations, fill_unspecified_columns_with_nulls)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m fill_unspecified_columns_with_nulls:\n\u001b[1;32m     42\u001b[0m     _transformations \u001b[39m=\u001b[39m add_nulls_for_unspecified_columns(\n\u001b[1;32m     43\u001b[0m         _transformations, schema, previously_existing_columns\u001b[39m=\u001b[39mdataframe\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m DataSet[schema](  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     reduce(\n\u001b[1;32m     48\u001b[0m         \u001b[39mlambda\u001b[39;49;00m acc, key: DataFrame\u001b[39m.\u001b[39;49mwithColumn(acc, key, _transformations[key]),\n\u001b[1;32m     49\u001b[0m         _transformations\u001b[39m.\u001b[39;49mkeys(),\n\u001b[1;32m     50\u001b[0m         dataframe,\n\u001b[1;32m     51\u001b[0m     )\u001b[39m.\u001b[39;49mselect(\u001b[39m*\u001b[39;49mschema\u001b[39m.\u001b[39;49mall_column_names())\n\u001b[1;32m     52\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/typedspark/lib/python3.11/site-packages/pyspark/sql/dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcols: \u001b[39m\"\u001b[39m\u001b[39mColumnOrName\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2992\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2993\u001b[0m \n\u001b[1;32m   2994\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[39m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3036\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jcols(\u001b[39m*\u001b[39;49mcols))\n\u001b[1;32m   3037\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/typedspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/typedspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`]."
     ]
    }
   ],
   "source": [
    "from typedspark import transform_to_schema\n",
    "from typedspark import register_schema_to_dataset\n",
    "from typedspark import Column, Schema, create_partially_filled_dataset\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.Builder().getOrCreate()\n",
    "\n",
    "\n",
    "class Person(Schema):\n",
    "    id: Column[IntegerType]\n",
    "    name: Column[StringType]\n",
    "    age: Column[IntegerType]\n",
    "\n",
    "\n",
    "class Job(Schema):\n",
    "    id: Column[IntegerType]\n",
    "    salary: Column[IntegerType]\n",
    "\n",
    "\n",
    "df_a = create_partially_filled_dataset(spark, Person, {Person.id: [1, 2, 3]})\n",
    "df_b = create_partially_filled_dataset(spark, Job, {Job.id: [1, 2, 3]})\n",
    "\n",
    "person = register_schema_to_dataset(df_a, Person)\n",
    "job = register_schema_to_dataset(df_b, Job)\n",
    "\n",
    "\n",
    "class PersonWithJob(Person, Job):\n",
    "    pass\n",
    "\n",
    "\n",
    "(\n",
    "    transform_to_schema(\n",
    "        df_a.join(\n",
    "            df_b,\n",
    "            person.id == job.id,\n",
    "        ),\n",
    "        PersonWithJob,\n",
    "        {\n",
    "            PersonWithJob.id: job.id,\n",
    "        },\n",
    "    ).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typedspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
