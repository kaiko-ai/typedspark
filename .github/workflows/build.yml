name: Python package

on: [pull_request]

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 200
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12", "3.13"]

    steps:
      - uses: actions/checkout@v5

      - uses: actions/setup-java@v5
        with:
          distribution: 'temurin'
          java-version: 21   # (Spark 3.5.x is happiest on 17; keep 21 if it works for you)

      # REPLACE setup-spark with a cache + install from Apache CDN
      - name: Cache Spark 3.5.3 (Hadoop 3)
        id: cache-spark
        uses: actions/cache@v4
        with:
          path: /opt/spark-3.5.3-bin-hadoop3
          key: spark-3.5.3-hadoop3-linux

      - name: Install Spark (if cache miss)
        if: steps.cache-spark.outputs.cache-hit != 'true'
        run: |
          curl -fsSL https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz -o /tmp/spark.tgz
          sudo mkdir -p /opt
          sudo tar -xzf /tmp/spark.tgz -C /opt

      - name: Set SPARK_HOME and PATH
        run: |
          echo "SPARK_HOME=/opt/spark-3.5.3-bin-hadoop3" >> $GITHUB_ENV
          echo "/opt/spark-3.5.3-bin-hadoop3/bin" >> $GITHUB_PATH

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Linting
        run: |
          flake8
          pylint typedspark
          mypy .
          pyright .
          bandit typedspark/**/*.py
          black --check .
          isort --check .
          docformatter --black -c **/*.py

      - name: Testing
        run: |
          # we run this test seperately, to ensure that it is run without an active Spark session
          python -m pytest -m no_spark_session
          coverage run -m pytest
          coverage report -m --fail-under 100

      - name: Run notebooks
        run: |
          for FILE in docs/*/*.ipynb; do
            BASE=$(basename $FILE)
            cp $FILE .
            jupyter nbconvert --to notebook $BASE --execute
          done
