name: Python package

on: [pull_request]

concurrency:
  group: "${{ github.workflow }}-${{ github.ref }}"
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 200
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12", "3.13"]
        pyspark-version: ["3.5.4", "4.1.0"]

    steps:
      - uses: actions/checkout@v6
        with:
          persist-credentials: false
      - uses: actions/setup-java@v5
        with:
          distribution: "temurin"
          java-version: 21
      - uses: vemonet/setup-spark@a3f00c89c1cb4e08f82b9f91c2d67201f02aa898 # v1.2.2
        with:
          spark-version: ${{ matrix.pyspark-version }}
          hadoop-version: "3"
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pyspark==${{ matrix.pyspark-version }}
      - name: Linting
        if: ${{ matrix.pyspark-version == '4.1.0' }}
        run: |
          flake8
          pylint typedspark
          mypy .
          pyright .
          bandit typedspark/**/*.py
          black --check .
          isort --check .
          docformatter --black -c **/*.py
      - name: Testing
        run: |
          # we run this test seperately, to ensure that it is run without an active Spark session
          python -m pytest -m no_spark_session

          coverage run -m pytest
          coverage report -m --fail-under 100
      - name: Run notebooks
        if: ${{ matrix.pyspark-version == '4.1.0' }}
        run: |
          for FILE in docs/*/*.ipynb; do
            BASE=$(basename $FILE)
            cp $FILE .
            jupyter nbconvert --to notebook $BASE --execute
          done
