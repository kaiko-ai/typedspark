name: Python package

on: [pull_request]

concurrency:
  group: "${{ github.workflow }}-${{ github.ref }}"
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 200
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12", "3.13"]
        pyspark-version: ["3.5.4", "4.1.0"]
        exclude:
          - python-version: "3.9"
            pyspark-version: "4.1.0"

    steps:
      - uses: actions/checkout@v6
        with:
          persist-credentials: false
      - uses: actions/setup-java@v5
        with:
          distribution: "temurin"
          java-version: 21
      - uses: vemonet/setup-spark@v1
        with:
          spark-version: ${{ matrix.pyspark-version }}
          hadoop-version: "3"
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pyspark==${{ matrix.pyspark-version }}
      - name: Start Spark Connect
        if: ${{ matrix.pyspark-version == '4.1.0' }}
        run: |
          "${SPARK_HOME}/sbin/start-connect-server.sh" \
            --conf spark.ui.enabled=false \
            --conf spark.ui.showConsoleProgress=false
          python - <<'PY'
          import socket
          import time

          for _ in range(30):
              try:
                  with socket.create_connection(("127.0.0.1", 15002), timeout=1):
                      break
              except OSError:
                  time.sleep(1)
          else:
              raise SystemExit("Spark Connect server did not start on port 15002")
          PY
      - name: Linting
        if: ${{ matrix.pyspark-version == '4.1.0' }}
        run: |
          flake8
          pylint typedspark
          mypy .
          pyright .
          bandit typedspark/**/*.py
          black --check .
          isort --check .
          docformatter --black -c **/*.py
      - name: Testing
        env:
          SPARK_CONNECT_URL: ${{ matrix.pyspark-version == '4.1.0' && 'sc://localhost:15002' || '' }}
        run: |
          # we run this test seperately, to ensure that it is run without an active Spark session
          python -m pytest -m no_spark_session

          coverage run -m pytest
          coverage report -m --fail-under 100
      - name: Run notebooks
        if: ${{ matrix.pyspark-version == '4.1.0' }}
        run: |
          for FILE in docs/*/*.ipynb; do
            BASE=$(basename $FILE)
            cp $FILE .
            jupyter nbconvert --to notebook $BASE --execute
          done
